{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Latent Semantic Analysis Lab\n",
    "\n",
    "Andrew Peabody apeab2@uis.edu\n",
    "\n",
    "This notebook performs a Latent Semantic Analysis concepts on a corpus (collection) of 'sci.crypt' newsgroup posts based on the lecture and provided examples.\n",
    "\n",
    "This particular LSA includes rather agressive data cleanup including:\n",
    " - fetch_20newsgroups strips the headers, footers, and quotes.\n",
    " - TfidfVectorizer handles the case conversion.\n",
    " - Removal of all digits (including binary) and underscores\n",
    " - A comprehensive stopwords list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 444,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Required Dependencies\n",
    "from sklearn.datasets import fetch_20newsgroups\n",
    "import nltk\n",
    "from nltk.corpus import stopwords\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.decomposition import TruncatedSVD"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 445,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[nltk_data] Downloading package stopwords to\n",
      "[nltk_data]     C:\\Users\\Andrew\\AppData\\Roaming\\nltk_data...\n",
      "[nltk_data]   Package stopwords is already up-to-date!\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "True"
      ]
     },
     "execution_count": 445,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#only required once to download stopwords\n",
    "nltk.download('stopwords')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 446,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Download the sci.crypt newsgroup posts\n",
    "categories = ['sci.crypt']\n",
    "dataset = fetch_20newsgroups(subset='all',shuffle=True, random_state=42, categories=categories,\n",
    "                            remove=('headers', 'footers', 'quotes'))\n",
    "corpus = dataset.data\n",
    "\n",
    "from string import digits\n",
    "corpus = [str(x).translate(None, digits+'_') for x in corpus]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 447,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#Additional custom stopwords to clean the data\n",
    "stopset = set(stopwords.words('english'))\n",
    "stopset.update(['lt','p','/p','br','amp','quot','field','font','normal','span','0px','rgb','style','51', \n",
    "                'spacing','text','helvetica','size','family', 'space', 'arial', 'height', 'indent', 'letter',\n",
    "                'line','none','sans','serif','transform','line','variant','weight','times', 'new','strong', 'video', 'title',\n",
    "                'white','word','letter', 'roman','0pt','16','color','12','14','21', 'neue', 'apple', 'class', 'com','edu',\n",
    "                'gov','org','one','get','nntp','uni','de','david','\\t','\\n','db','amanda','steve','ellisun','carl','bontchev',\n",
    "                'fbihh','informatik','aa'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 448,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#These might be useful to troubleshoot the corpus or stopword set\n",
    "#corpus[0]\n",
    "#stopset"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### TF-IDF Vectorizing\n",
    "\n",
    "Use scikit-learn's TF-IDF vectorizer to take the corpus and convert each document into a sparse matrix of TFIDF Features..."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 449,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words=stopset, lowercase=True,\n",
    "                                 use_idf=True, ngram_range=(1, 3))\n",
    "X = vectorizer.fit_transform(corpus)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 450,
   "metadata": {
    "collapsed": false
   },
   "outputs": [],
   "source": [
    "#This might be useful to check the scoring of the first document\n",
    "#X[0]\n",
    "#This can be used to check the number of documents and/or features.\n",
    "#X.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Perform Latent Semantic Analysis\n",
    "\n",
    "Perform the Truncated Singular Value Decomposition.  As these are newgroups threads I've decided to go with n_components of ~1%, this also keeps the RAM usage and CPU time reasonable."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 455,
   "metadata": {
    "collapsed": false,
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "TruncatedSVD(algorithm='randomized', n_components=10, n_iter=100,\n",
       "       random_state=None, tol=0.0)"
      ]
     },
     "execution_count": 455,
     "metadata": {},
     "output_type": "execute_result"
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n"
     ]
    }
   ],
   "source": [
    "lsa = TruncatedSVD(n_components=10, n_iter=100)\n",
    "lsa.fit(X)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Discovered Concepts"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 456,
   "metadata": {
    "collapsed": false
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Concept 0:\n",
      "key\n",
      "encryption\n",
      "chip\n",
      "would\n",
      "clipper\n",
      "government\n",
      "keys\n",
      "use\n",
      "law\n",
      "escrow\n",
      " \n",
      "Concept 1:\n",
      "encryption\n",
      "law\n",
      "americans\n",
      "technology\n",
      "industry\n",
      "devices\n",
      "encryption technology\n",
      "law enforcement\n",
      "key escrow\n",
      "administration\n",
      " \n",
      "Concept 2:\n",
      "internet\n",
      "phone\n",
      "government\n",
      "pub\n",
      "security\n",
      "email\n",
      "privacy\n",
      "anonymous\n",
      "eff\n",
      "know\n",
      " \n",
      "Concept 3:\n",
      "key\n",
      "ripem\n",
      "use\n",
      "encrypted\n",
      "also\n",
      "aachen math\n",
      "security\n",
      "internet\n",
      "like\n",
      "two\n",
      " \n",
      "Concept 4:\n",
      "key\n",
      "would\n",
      "ripem\n",
      "government\n",
      "administration\n",
      "use\n",
      "public\n",
      "rsa\n",
      "pq\n",
      "law enforcement\n",
      " \n",
      "Concept 5:\n",
      "encryption\n",
      "anonymous\n",
      "anon\n",
      "posting\n",
      "anonymity\n",
      "penet\n",
      "clipper\n",
      "two\n",
      "penet fi\n",
      "administration\n",
      " \n",
      "Concept 6:\n",
      "like\n",
      "use\n",
      "law\n",
      "chip\n",
      "public\n",
      "escrow\n",
      "used\n",
      "algorithm\n",
      "serial number\n",
      "need\n",
      " \n",
      "Concept 7:\n",
      "use\n",
      "clipper\n",
      "escrow\n",
      "government\n",
      "aachen ftp site\n",
      "time\n",
      "information\n",
      "agencies\n",
      "many\n",
      "device\n",
      " \n",
      "Concept 8:\n",
      "chip\n",
      "could\n",
      "escrow\n",
      "use\n",
      "would\n",
      "law\n",
      "right\n",
      "also\n",
      "keys\n",
      "might\n",
      " \n",
      "Concept 9:\n",
      "key\n",
      "clipper\n",
      "government\n",
      "chip\n",
      "number\n",
      "people\n",
      "nsa\n",
      "aachen ftp site\n",
      "much\n",
      "like\n",
      " \n"
     ]
    }
   ],
   "source": [
    "terms = vectorizer.get_feature_names()\n",
    "for i, comp in enumerate(lsa.components_): \n",
    "    termsInComp = zip (terms,comp)\n",
    "    sortedTerms =  sorted(termsInComp, key=lambda x: x[1], reverse=True) [:10]\n",
    "    print \"Concept %d:\" % i\n",
    "    for term in sortedTerms:\n",
    "        print term[0]\n",
    "    print \" \""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "anaconda-cloud": {},
  "kernelspec": {
   "display_name": "Python [Root]",
   "language": "python",
   "name": "Python [Root]"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
